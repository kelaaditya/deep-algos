{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, activation_func, W_init, b_init):\n",
    "        '''\n",
    "        output = activation(WX+b)\n",
    "        X is the data matrix (each column is one example)\n",
    "        W takes 'n_in' inputs from the previous layer (the number of previous layers nodes)\n",
    "        and has 'n_out' outputs (the number of the next layers nodes)\n",
    "        \n",
    "        parameters:\n",
    "        W_init: initial weight values\n",
    "            np.ndarray\n",
    "        b_init: initial bias value\n",
    "            np.ndarray\n",
    "        activation_func: activation function for output of layer\n",
    "            theano.tensor.elemwise.Elemwise\n",
    "        '''\n",
    "        n_out, n_in = W_init.shape\n",
    "        self.W = theano.shared(value=W_init.astype(theano.config.floatX), borrow=True)\n",
    "        #borrow=True as we allow Theano to use memory for this object (make faster)\n",
    "        self.b = theano.shared(value=b_init.reshape(n_out, 1).astype(theano.config.floatX), \n",
    "                               borrow=True, \n",
    "                               broadcastable=(False, True))\n",
    "        self.activation_func = activation_func\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def output(self, X):\n",
    "        '''\n",
    "        Gives the output: activation(WX+b)\n",
    "        '''\n",
    "        pre_activation = T.dot(self.W, X) + self.b\n",
    "        return(self.activation_func(pre_activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mlp:\n",
    "    def __init__(self, n_layer, output_activation_func, hidden_activation_func, W_init, b_init):\n",
    "        '''\n",
    "        output of sequential layers\n",
    "        \n",
    "        parameters:\n",
    "        n_layer: number of layers\n",
    "            np.ndarray\n",
    "        output_activation_func: activation function of the output layer\n",
    "        '''\n",
    "        self.layers = [layer(hidden_activation_func, W_init, b_init) for i in range(n_layer-1)]\n",
    "        self.layers.append(layer(output_activation_func, W_init, b_init))\n",
    "            \n",
    "        self.params = [item for layer in self.layers for item in layer.params]\n",
    "        #mlp.params is a list of parameters, not a list of lists of parameters\n",
    "    \n",
    "    def output(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.output(X)\n",
    "        return(X)\n",
    "    \n",
    "    def error(self, X, y):\n",
    "        return(T.sum((y-self.output(X))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_update(loss, params, eta):\n",
    "    updated_param = []\n",
    "    for param in params:\n",
    "        step = -eta*T.grad(loss, param)\n",
    "        updated_param.append((param, param + step))\n",
    "    return(updated_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Choose appropriate W and b\n",
    "W_init = np.zeros((800,784))\n",
    "b_init = np.zeros(800)\n",
    "\n",
    "#input measurement matrix\n",
    "X = T.matrix('X')\n",
    "\n",
    "#classification vector\n",
    "y = T.vector('y')\n",
    "\n",
    "#initialize 3 layer MLP (1 hidden layers)\n",
    "MLP = mlp(3, T.nnet.sigmoid, T.nnet.relu, W_init, b_init)\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "cost = MLP.error(X, y)\n",
    "training = theano.function([X, y], cost, updates=gradient_update(cost, MLP.params, eta))\n",
    "y_hat = theano.function([X], MLP.output(X))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
